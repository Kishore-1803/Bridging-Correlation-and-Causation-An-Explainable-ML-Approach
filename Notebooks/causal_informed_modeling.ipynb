{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16a8148",
   "metadata": {},
   "source": [
    "# Causal-Informed Model Training\n",
    "\n",
    "This notebook demonstrates how to use causal effects directly in model training, comparing:\n",
    "1. Traditional Ridge/GB training (baseline)\n",
    "2. Causal-informed Ridge training (using causal effects as priors)\n",
    "3. Causal-informed GB training (using causal effects as constraints)\n",
    "\n",
    "**Goal:** See if incorporating causal knowledge improves model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d109c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Causal effects loaded from file\n",
      "\n",
      "üìä Data Summary:\n",
      "   ‚Ä¢ Training samples: 1002\n",
      "   ‚Ä¢ Test samples: 251\n",
      "   ‚Ä¢ Features: ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\n",
      "\n",
      "üîç Causal Effects:\n",
      "   ‚Ä¢ smoker: $23808.21\n",
      "   ‚Ä¢ age: $257.41\n",
      "   ‚Ä¢ bmi: $332.04\n",
      "   ‚Ä¢ children: $478.44\n",
      "   ‚Ä¢ sex: $-131.31\n",
      "   ‚Ä¢ region: $-352.96\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_insurance_data.csv')\n",
    "\n",
    "try:\n",
    "    with open('causal_effects.json', 'r') as f:\n",
    "        causal_data = json.load(f)\n",
    "    if 'causal_effects' in causal_data:\n",
    "        causal_effects = causal_data['causal_effects']\n",
    "    else:\n",
    "        causal_effects = causal_data\n",
    "    print(\"‚úÖ Causal effects loaded from file\")\n",
    "except FileNotFoundError:\n",
    "    causal_effects = {\n",
    "        'age': 257.41,\n",
    "        'sex': -131.31, \n",
    "        'bmi': 332.04,\n",
    "        'children': 478.44,\n",
    "        'smoker': 23808.21,\n",
    "        'region': -352.96\n",
    "    }\n",
    "    print(\"‚ö†Ô∏è Using predefined causal effects\")\n",
    "\n",
    "X = df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n",
    "y = df['charges']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train)}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test)}\")\n",
    "print(f\"   ‚Ä¢ Features: {list(X.columns)}\")\n",
    "\n",
    "print(f\"\\nüîç Causal Effects:\")\n",
    "for feature, effect in causal_effects.items():\n",
    "    print(f\"   ‚Ä¢ {feature}: ${effect:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696477f",
   "metadata": {},
   "source": [
    "## Method 1: Traditional Model Training (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b32904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß METHOD 1: TRADITIONAL MODEL TRAINING\n",
      "============================================================\n",
      "‚úÖ Traditional Ridge Results:\n",
      "   ‚Ä¢ R¬≤ Score: 0.7230\n",
      "   ‚Ä¢ MAE: $4098\n",
      "\n",
      "‚úÖ Traditional GB Results:\n",
      "   ‚Ä¢ R¬≤ Score: 0.9307\n",
      "   ‚Ä¢ MAE: $1252\n",
      "\n",
      "üìä Traditional Model Coefficients/Importances:\n",
      "Feature    Ridge Coeff  GB Importance  Causal Effect\n",
      "-------------------------------------------------------\n",
      "age        229.31       0.1089         $257        \n",
      "sex        96.21        0.0020         $-131       \n",
      "bmi        299.51       0.1638         $332        \n",
      "children   417.66       0.0069         $478        \n",
      "smoker     14675.35     0.7143         $23808      \n",
      "region     -303.08      0.0040         $-353       \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîß METHOD 1: TRADITIONAL MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "traditional_ridge = Ridge(alpha=100, random_state=42)\n",
    "traditional_ridge.fit(X_train, y_train)\n",
    "trad_ridge_pred = traditional_ridge.predict(X_test)\n",
    "trad_ridge_r2 = r2_score(y_test, trad_ridge_pred)\n",
    "trad_ridge_mae = mean_absolute_error(y_test, trad_ridge_pred)\n",
    "\n",
    "traditional_gb = GradientBoostingRegressor(\n",
    "    n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42\n",
    ")\n",
    "traditional_gb.fit(X_train, y_train)\n",
    "trad_gb_pred = traditional_gb.predict(X_test)\n",
    "trad_gb_r2 = r2_score(y_test, trad_gb_pred)\n",
    "trad_gb_mae = mean_absolute_error(y_test, trad_gb_pred)\n",
    "\n",
    "print(f\"‚úÖ Traditional Ridge Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {trad_ridge_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: ${trad_ridge_mae:.0f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Traditional GB Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {trad_gb_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: ${trad_gb_mae:.0f}\")\n",
    "\n",
    "print(f\"\\nüìä Traditional Model Coefficients/Importances:\")\n",
    "print(f\"{'Feature':<10} {'Ridge Coeff':<12} {'GB Importance':<14} {'Causal Effect'}\")\n",
    "print(\"-\" * 55)\n",
    "for i, feature in enumerate(X.columns):\n",
    "    ridge_coeff = traditional_ridge.coef_[i]\n",
    "    gb_importance = traditional_gb.feature_importances_[i]\n",
    "    causal_effect = causal_effects.get(feature, 0)\n",
    "    print(f\"{feature:<10} {ridge_coeff:<12.2f} {gb_importance:<14.4f} ${causal_effect:<11.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680aab7",
   "metadata": {},
   "source": [
    "## Method 2: Causal-Informed Ridge Training (Strong Constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4446b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß† METHOD 2: CAUSAL-INFORMED RIDGE TRAINING\n",
      "============================================================\n",
      "‚úÖ Causal-Informed Ridge Results:\n",
      "   ‚Ä¢ R¬≤ Score: 0.7609\n",
      "   ‚Ä¢ MAE: $4298\n",
      "   ‚Ä¢ Causal Weight: 70% (strong causal constraints)\n",
      "   ‚Ä¢ Coefficient-Causal Alignment: 0.995\n",
      "\n",
      "üìä Causal-Informed Ridge Coefficients:\n",
      "   ‚Ä¢ age: 960.70 (causal: $257)\n",
      "   ‚Ä¢ sex: -53.93 (causal: $-131)\n",
      "   ‚Ä¢ bmi: 594.51 (causal: $332)\n",
      "   ‚Ä¢ children: 282.45 (causal: $478)\n",
      "   ‚Ä¢ smoker: 9134.68 (causal: $23808)\n",
      "   ‚Ä¢ region: -171.51 (causal: $-353)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß† METHOD 2: CAUSAL-INFORMED RIDGE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "class CausalInformedRidge:\n",
    "    def __init__(self, causal_effects, alpha=100, causal_weight=0.7):\n",
    "        self.causal_effects = causal_effects\n",
    "        self.alpha = alpha\n",
    "        self.causal_weight = causal_weight\n",
    "        self.base_model = Ridge(alpha=alpha, random_state=42)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.base_model.fit(X, y)\n",
    "        \n",
    "        causal_vector = np.array([self.causal_effects.get(col, 0) for col in X.columns])\n",
    "        \n",
    "        if np.linalg.norm(causal_vector) > 0:\n",
    "            causal_vector = causal_vector / np.linalg.norm(causal_vector) * np.linalg.norm(self.base_model.coef_)\n",
    "        \n",
    "        self.coef_ = (1 - self.causal_weight) * self.base_model.coef_ + self.causal_weight * causal_vector\n",
    "        self.intercept_ = self.base_model.intercept_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "causal_ridge = CausalInformedRidge(causal_effects, alpha=100, causal_weight=0.7)\n",
    "causal_ridge.fit(X_train_scaled_df, y_train)\n",
    "causal_ridge_pred = causal_ridge.predict(X_test_scaled_df)\n",
    "causal_ridge_r2 = r2_score(y_test, causal_ridge_pred)\n",
    "causal_ridge_mae = mean_absolute_error(y_test, causal_ridge_pred)\n",
    "\n",
    "print(f\"‚úÖ Causal-Informed Ridge Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {causal_ridge_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: ${causal_ridge_mae:.0f}\")\n",
    "print(f\"   ‚Ä¢ Causal Weight: 70% (strong causal constraints)\")\n",
    "\n",
    "causal_vector = [causal_effects.get(col, 0) for col in X.columns]\n",
    "causal_normalized = [c/max([abs(x) for x in causal_vector]) if max([abs(x) for x in causal_vector]) > 0 else 0 for c in causal_vector]\n",
    "coeff_normalized = [c/max([abs(x) for x in causal_ridge.coef_]) if max([abs(x) for x in causal_ridge.coef_]) > 0 else 0 for c in causal_ridge.coef_]\n",
    "ridge_alignment = np.corrcoef(coeff_normalized, causal_normalized)[0,1] if len(set(coeff_normalized)) > 1 and len(set(causal_normalized)) > 1 else 0\n",
    "\n",
    "print(f\"   ‚Ä¢ Coefficient-Causal Alignment: {ridge_alignment:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Causal-Informed Ridge Coefficients:\")\n",
    "for i, feature in enumerate(X.columns):\n",
    "    coeff = causal_ridge.coef_[i]\n",
    "    causal_effect = causal_effects.get(feature, 0)\n",
    "    print(f\"   ‚Ä¢ {feature}: {coeff:.2f} (causal: ${causal_effect:.0f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbc1e3",
   "metadata": {},
   "source": [
    "## Method 3: Causal-Informed Gradient Boosting (Strong Constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a2869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üå≥ METHOD 3: CAUSAL-INFORMED GRADIENT BOOSTING\n",
      "============================================================\n",
      "‚úÖ Causal-Informed GB Results:\n",
      "   ‚Ä¢ R¬≤ Score: 0.9311\n",
      "   ‚Ä¢ MAE: $1250\n",
      "   ‚Ä¢ Method: Strong causal scaling + enhanced features\n",
      "   ‚Ä¢ Feature Importance Alignment: 0.970\n",
      "\n",
      "üìä Enhanced Feature Set:\n",
      "   ‚Ä¢ Original features: 6\n",
      "   ‚Ä¢ Total features (with causal): 7\n",
      "   ‚Ä¢ Strong causal scaling applied to all features\n",
      "   ‚Ä¢ Enhanced features for top causal drivers\n",
      "‚úÖ Causal-Informed GB model saved as 'causal_informed_gb_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üå≥ METHOD 3: CAUSAL-INFORMED GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class CausalInformedGB:\n",
    "    def __init__(self, causal_effects, n_estimators=200, max_depth=6, learning_rate=0.1, causal_weight=0.9):\n",
    "        self.causal_effects = causal_effects\n",
    "        self.causal_weight = causal_weight\n",
    "        self.base_model = GradientBoostingRegressor(\n",
    "            n_estimators=n_estimators, max_depth=max_depth, \n",
    "            learning_rate=learning_rate, random_state=42\n",
    "        )\n",
    "        \n",
    "    def _create_causal_features(self, X):\n",
    "        X_causal = X.copy()\n",
    "        \n",
    "        causal_magnitudes = [abs(self.causal_effects.get(col, 0)) for col in X.columns]\n",
    "        max_causal = max(causal_magnitudes)\n",
    "        \n",
    "        for i, col in enumerate(X.columns):\n",
    "            causal_effect = abs(self.causal_effects.get(col, 0))\n",
    "            \n",
    "            if max_causal > 0:\n",
    "                scale_factor = 0.1 + 1.9 * (causal_effect / max_causal)\n",
    "                X_causal[col] = X[col] * scale_factor\n",
    "                \n",
    "            if causal_effect > max_causal * 0.5:\n",
    "                X_causal[f'{col}_causal_enhanced'] = X[col] * (causal_effect / max_causal) * 2\n",
    "        \n",
    "        return X_causal\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_causal = self._create_causal_features(X)\n",
    "        \n",
    "        self.base_model.fit(X_causal, y)\n",
    "        self.feature_names_ = X_causal.columns\n",
    "        self.original_features = X.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_causal = self._create_causal_features(X)\n",
    "        return self.base_model.predict(X_causal)\n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.base_model.feature_importances_\n",
    "    \n",
    "    def get_original_feature_importances(self):\n",
    "        original_importances = []\n",
    "        for col in self.original_features:\n",
    "            col_idx = list(self.feature_names_).index(col)\n",
    "            importance = self.base_model.feature_importances_[col_idx]\n",
    "            \n",
    "            enhanced_col = f'{col}_causal_enhanced'\n",
    "            if enhanced_col in self.feature_names_:\n",
    "                enhanced_idx = list(self.feature_names_).index(enhanced_col)\n",
    "                importance += self.base_model.feature_importances_[enhanced_idx]\n",
    "            \n",
    "            original_importances.append(importance)\n",
    "        \n",
    "        return np.array(original_importances)\n",
    "\n",
    "causal_gb = CausalInformedGB(causal_effects, causal_weight=1.0)\n",
    "causal_gb.fit(X_train, y_train)\n",
    "causal_gb_pred = causal_gb.predict(X_test)\n",
    "causal_gb_r2 = r2_score(y_test, causal_gb_pred)\n",
    "causal_gb_mae = mean_absolute_error(y_test, causal_gb_pred)\n",
    "\n",
    "print(f\"‚úÖ Causal-Informed GB Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {causal_gb_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: ${causal_gb_mae:.0f}\")\n",
    "print(f\"   ‚Ä¢ Method: Strong causal scaling + enhanced features\")\n",
    "\n",
    "causal_magnitudes = [abs(causal_effects[f]) for f in X.columns]\n",
    "causal_normalized = np.array(causal_magnitudes) / max(causal_magnitudes)\n",
    "gb_importances = causal_gb.get_original_feature_importances()\n",
    "gb_normalized = gb_importances / max(gb_importances)\n",
    "gb_alignment = np.corrcoef(causal_normalized, gb_normalized)[0,1]\n",
    "\n",
    "print(f\"   ‚Ä¢ Feature Importance Alignment: {gb_alignment:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Enhanced Feature Set:\")\n",
    "print(f\"   ‚Ä¢ Original features: {len(X.columns)}\")\n",
    "print(f\"   ‚Ä¢ Total features (with causal): {len(causal_gb.feature_names_)}\")\n",
    "print(f\"   ‚Ä¢ Strong causal scaling applied to all features\")\n",
    "print(f\"   ‚Ä¢ Enhanced features for top causal drivers\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(causal_gb, \"causal_informed_gb_model.joblib\")\n",
    "print(\"‚úÖ Causal-Informed GB model saved as 'causal_informed_gb_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef00ac",
   "metadata": {},
   "source": [
    "## Method 4: Causal Effects as Direct Predictions (Upper Bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ METHOD 4: PURE CAUSAL PREDICTION (UPPER BOUND)\n",
      "============================================================\n",
      "‚úÖ Pure Causal Prediction Results:\n",
      "   ‚Ä¢ R¬≤ Score: 0.8119\n",
      "   ‚Ä¢ MAE: $3928\n",
      "   ‚Ä¢ Method: Linear combination of causal effects only\n",
      "   ‚Ä¢ Note: This represents the theoretical upper bound if\n",
      "           causal effects captured all relationships perfectly\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ METHOD 4: PURE CAUSAL PREDICTION (UPPER BOUND)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def causal_predict(X, causal_effects, baseline_cost=5000):\n",
    "    predictions = np.full(len(X), baseline_cost)\n",
    "    \n",
    "    for i, feature in enumerate(X.columns):\n",
    "        causal_effect = causal_effects.get(feature, 0)\n",
    "        \n",
    "        if feature == 'smoker':\n",
    "            predictions += X[feature] * causal_effect\n",
    "        elif feature in ['sex', 'region']:\n",
    "            predictions += X[feature] * causal_effect\n",
    "        else:\n",
    "            if feature == 'age':\n",
    "                predictions += (X[feature] - 30) * causal_effect\n",
    "            elif feature == 'bmi':\n",
    "                predictions += (X[feature] - 25) * causal_effect\n",
    "            elif feature == 'children':\n",
    "                predictions += X[feature] * causal_effect\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "pure_causal_pred = causal_predict(X_test, causal_effects)\n",
    "pure_causal_r2 = r2_score(y_test, pure_causal_pred)\n",
    "pure_causal_mae = mean_absolute_error(y_test, pure_causal_pred)\n",
    "\n",
    "print(f\"‚úÖ Pure Causal Prediction Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {pure_causal_r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: ${pure_causal_mae:.0f}\")\n",
    "print(f\"   ‚Ä¢ Method: Linear combination of causal effects only\")\n",
    "print(f\"   ‚Ä¢ Note: This represents the theoretical upper bound if\")\n",
    "print(f\"           causal effects captured all relationships perfectly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b277101",
   "metadata": {},
   "source": [
    "## Results Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e88786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPREHENSIVE RESULTS COMPARISON\n",
      "================================================================================\n",
      "Method                    R¬≤ Score   MAE        R¬≤ Rank  MAE Rank\n",
      "----------------------------------------------------------------------\n",
      "Traditional Ridge         0.7230     $4098      5        4\n",
      "Traditional GB            0.9307     $1252      2        2\n",
      "Causal-Informed Ridge     0.7609     $4298      4        5\n",
      "Causal-Informed GB        0.9311     $1250      1        1\n",
      "Pure Causal Effects       0.8119     $3928      3        3\n",
      "\n",
      "üèÜ WINNERS:\n",
      "   ‚Ä¢ Best R¬≤ Score: Causal-Informed GB (0.9311)\n",
      "   ‚Ä¢ Best MAE: Causal-Informed GB ($1250)\n",
      "\n",
      "üìà CAUSAL-INFORMED vs TRADITIONAL:\n",
      "   Ridge R¬≤ Change: +5.2%\n",
      "   Ridge MAE Change: -4.9%\n",
      "   GB R¬≤ Change: +0.0%\n",
      "   GB MAE Change: +0.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {\n",
    "    'Method': [\n",
    "        'Traditional Ridge',\n",
    "        'Traditional GB', \n",
    "        'Causal-Informed Ridge',\n",
    "        'Causal-Informed GB',\n",
    "        'Pure Causal Effects'\n",
    "    ],\n",
    "    'R¬≤ Score': [\n",
    "        trad_ridge_r2,\n",
    "        trad_gb_r2,\n",
    "        causal_ridge_r2,\n",
    "        causal_gb_r2,\n",
    "        pure_causal_r2\n",
    "    ],\n",
    "    'MAE': [\n",
    "        trad_ridge_mae,\n",
    "        trad_gb_mae,\n",
    "        causal_ridge_mae,\n",
    "        causal_gb_mae,\n",
    "        pure_causal_mae\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['R¬≤ Rank'] = results_df['R¬≤ Score'].rank(ascending=False).astype(int)\n",
    "results_df['MAE Rank'] = results_df['MAE'].rank(ascending=True).astype(int)\n",
    "\n",
    "print(f\"{'Method':<25} {'R¬≤ Score':<10} {'MAE':<10} {'R¬≤ Rank':<8} {'MAE Rank'}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Method']:<25} {row['R¬≤ Score']:<10.4f} ${row['MAE']:<9.0f} {row['R¬≤ Rank']:<8} {row['MAE Rank']}\")\n",
    "\n",
    "best_r2_idx = results_df['R¬≤ Score'].idxmax()\n",
    "best_mae_idx = results_df['MAE'].idxmin()\n",
    "best_r2_method = results_df.loc[best_r2_idx, 'Method']\n",
    "best_mae_method = results_df.loc[best_mae_idx, 'Method']\n",
    "\n",
    "print(f\"\\nüèÜ WINNERS:\")\n",
    "print(f\"   ‚Ä¢ Best R¬≤ Score: {best_r2_method} ({results_df.loc[best_r2_idx, 'R¬≤ Score']:.4f})\")\n",
    "print(f\"   ‚Ä¢ Best MAE: {best_mae_method} (${results_df.loc[best_mae_idx, 'MAE']:.0f})\")\n",
    "\n",
    "print(f\"\\nüìà CAUSAL-INFORMED vs TRADITIONAL:\")\n",
    "\n",
    "ridge_r2_improvement = (causal_ridge_r2 - trad_ridge_r2) / trad_ridge_r2 * 100\n",
    "ridge_mae_improvement = (trad_ridge_mae - causal_ridge_mae) / trad_ridge_mae * 100\n",
    "print(f\"   Ridge R¬≤ Change: {ridge_r2_improvement:+.1f}%\")\n",
    "print(f\"   Ridge MAE Change: {ridge_mae_improvement:+.1f}%\")\n",
    "\n",
    "gb_r2_improvement = (causal_gb_r2 - trad_gb_r2) / trad_gb_r2 * 100\n",
    "gb_mae_improvement = (trad_gb_mae - causal_gb_mae) / trad_gb_mae * 100\n",
    "print(f\"   GB R¬≤ Change: {gb_r2_improvement:+.1f}%\")\n",
    "print(f\"   GB MAE Change: {gb_mae_improvement:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîÆ SAMPLE PREDICTIONS COMPARISON\n",
      "================================================================================\n",
      "Actual   Trad Ridge   Causal Ridge  Trad GB    Causal GB    Pure Causal\n",
      "---------------------------------------------------------------------------\n",
      "$14475   $15863       $9540         $14119     $14118       $16653     \n",
      "$38345   $22147       $29742        $40321     $40320       $30781     \n",
      "$42761   $27611       $31478        $42881     $42881       $36954     \n",
      "$6986    $7696        $7169         $7035      $7029        $7343      \n",
      "$39241   $23960       $30270        $38878     $38878       $32825     \n",
      "$18328   $19356       $28883        $22268     $21707       $27938     \n",
      "$2639    $4797        $6073         $2648      $2648        $4033      \n",
      "$11674   $14134       $8964         $11591     $11591       $14698     \n",
      "\n",
      "üéâ EXPERIMENT COMPLETE!\n",
      "   You now have evidence of whether causal-informed\n",
      "   modeling improves performance for your specific dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÆ SAMPLE PREDICTIONS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"{'Actual':<8} {'Trad Ridge':<12} {'Causal Ridge':<13} {'Trad GB':<10} {'Causal GB':<12} {'Pure Causal'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i in range(min(8, len(y_test))):\n",
    "    actual = y_test.iloc[i]\n",
    "    trad_r = trad_ridge_pred[i]\n",
    "    causal_r = causal_ridge_pred.iloc[i] if hasattr(causal_ridge_pred, 'iloc') else causal_ridge_pred[i]\n",
    "    trad_g = trad_gb_pred[i]\n",
    "    causal_g = causal_gb_pred[i]\n",
    "    pure_c = pure_causal_pred.iloc[i] if hasattr(pure_causal_pred, 'iloc') else pure_causal_pred[i]\n",
    "    \n",
    "    print(f\"${actual:<7.0f} ${trad_r:<11.0f} ${causal_r:<12.0f} ${trad_g:<9.0f} ${causal_g:<11.0f} ${pure_c:<10.0f}\")\n",
    "\n",
    "print(f\"\\nüéâ EXPERIMENT COMPLETE!\")\n",
    "print(f\"   You now have evidence of whether causal-informed\")\n",
    "print(f\"   modeling improves performance for your specific dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä INTERPRETABILITY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "=== Ridge Coefficient Comparison ===\n",
      "Feature     Traditional    Causal-Informed   Causal Effect\n",
      "------------------------------------------------------------\n",
      "age              229.31             960.70          257.41\n",
      "sex               96.21             -53.93         -131.31\n",
      "bmi              299.51             594.51          332.04\n",
      "children         417.66             282.45          478.44\n",
      "smoker         14675.35            9134.68        23808.21\n",
      "region          -303.08            -171.51         -352.96\n",
      "\n",
      "   üéØ Ridge Coefficient-Causal Alignment: 0.995\n",
      "\n",
      "=== GB Feature Importance Comparison ===\n",
      "Feature     Traditional    Causal-Informed   Causal Effect\n",
      "------------------------------------------------------------\n",
      "age              0.1089             0.1092          257.41\n",
      "sex              0.0020             0.0021          131.31\n",
      "bmi              0.1638             0.1638          332.04\n",
      "children         0.0069             0.0068          478.44\n",
      "smoker           0.7143             0.7143        23808.21\n",
      "region           0.0040             0.0038          352.96\n",
      "\n",
      "   üéØ GB Feature Importance-Causal Alignment: 0.970\n",
      "\n",
      "üí° INTERPRETABILITY INSIGHTS:\n",
      "   ‚úÖ Ridge: Positive alignment achieved - coefficients match causal effects\n",
      "   ‚úÖ GB: Positive alignment achieved - importances match causal effects\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä INTERPRETABILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n=== Ridge Coefficient Comparison ===\")\n",
    "print(f\"{'Feature':<10} {'Traditional':>12} {'Causal-Informed':>18} {'Causal Effect':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for i, feature in enumerate(X.columns):\n",
    "    trad = traditional_ridge.coef_[i]\n",
    "    causal = causal_ridge.coef_[i]\n",
    "    effect = causal_effects.get(feature, 0)\n",
    "    print(f\"{feature:<10} {trad:>12.2f} {causal:>18.2f} {effect:>15.2f}\")\n",
    "\n",
    "print(f\"\\n   üéØ Ridge Coefficient-Causal Alignment: {ridge_alignment:.3f}\")\n",
    "\n",
    "print(\"\\n=== GB Feature Importance Comparison ===\")\n",
    "print(f\"{'Feature':<10} {'Traditional':>12} {'Causal-Informed':>18} {'Causal Effect':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for i, feature in enumerate(X.columns):\n",
    "    trad = traditional_gb.feature_importances_[i]\n",
    "    causal = causal_gb.get_original_feature_importances()[i]\n",
    "    effect = abs(causal_effects.get(feature, 0))\n",
    "    print(f\"{feature:<10} {trad:>12.4f} {causal:>18.4f} {effect:>15.2f}\")\n",
    "\n",
    "print(f\"\\n   üéØ GB Feature Importance-Causal Alignment: {gb_alignment:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETABILITY INSIGHTS:\")\n",
    "if ridge_alignment > 0:\n",
    "    print(f\"   ‚úÖ Ridge: Positive alignment achieved - coefficients match causal effects\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Ridge: Negative alignment - coefficients oppose causal ranking\")\n",
    "\n",
    "if gb_alignment > 0:\n",
    "    print(f\"   ‚úÖ GB: Positive alignment achieved - importances match causal effects\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  GB: Negative alignment - importances oppose causal ranking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31454138",
   "metadata": {},
   "source": [
    "## üìã Final Conclusions\n",
    "\n",
    "**Comparing Traditional vs Causal-Informed Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6801c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÜ CAUSAL-INFORMED MODELING CONCLUSIONS\n",
      "================================================================================\n",
      "\n",
      "üìà PERFORMANCE COMPARISON:\n",
      "   Traditional Ridge - MAE: $4098, R¬≤: 0.723\n",
      "   Causal Ridge      - MAE: $4298, R¬≤: 0.761\n",
      "   Traditional GB    - MAE: $1252, R¬≤: 0.931\n",
      "   Causal GB         - MAE: $1250, R¬≤: 0.931\n",
      "\n",
      "üß† INTERPRETABILITY ALIGNMENT:\n",
      "   Ridge Coefficient-Causal Alignment:     0.995\n",
      "   GB Feature Importance-Causal Alignment: 0.970\n",
      "\n",
      "üéØ KEY INSIGHTS:\n",
      "   ‚Ä¢ Strong causal constraints (70% regularization) improve interpretability\n",
      "   ‚Ä¢ Trade-off exists between pure prediction accuracy and causal alignment\n",
      "   ‚Ä¢ Causal-informed models provide more business-interpretable feature rankings\n",
      "\n",
      "‚úÖ SUCCESS: Both models achieve positive causal alignment!\n",
      "   Models now rank features similarly to true causal effects\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ CAUSAL-INFORMED MODELING CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE COMPARISON:\")\n",
    "print(f\"   Traditional Ridge - MAE: ${trad_ridge_mae:.0f}, R¬≤: {trad_ridge_r2:.3f}\")\n",
    "print(f\"   Causal Ridge      - MAE: ${causal_ridge_mae:.0f}, R¬≤: {causal_ridge_r2:.3f}\")\n",
    "print(f\"   Traditional GB    - MAE: ${trad_gb_mae:.0f}, R¬≤: {trad_gb_r2:.3f}\")\n",
    "print(f\"   Causal GB         - MAE: ${causal_gb_mae:.0f}, R¬≤: {causal_gb_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nüß† INTERPRETABILITY ALIGNMENT:\")\n",
    "print(f\"   Ridge Coefficient-Causal Alignment:     {ridge_alignment:.3f}\")\n",
    "print(f\"   GB Feature Importance-Causal Alignment: {gb_alignment:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Strong causal constraints (70% regularization) improve interpretability\")\n",
    "print(f\"   ‚Ä¢ Trade-off exists between pure prediction accuracy and causal alignment\")\n",
    "print(f\"   ‚Ä¢ Causal-informed models provide more business-interpretable feature rankings\")\n",
    "\n",
    "if ridge_alignment > 0 and gb_alignment > 0:\n",
    "    print(f\"\\n‚úÖ SUCCESS: Both models achieve positive causal alignment!\")\n",
    "    print(f\"   Models now rank features similarly to true causal effects\")\n",
    "elif ridge_alignment > 0:\n",
    "    print(f\"\\nüéØ PARTIAL SUCCESS: Ridge achieves causal alignment\")\n",
    "elif gb_alignment > 0:\n",
    "    print(f\"\\nüéØ PARTIAL SUCCESS: Gradient Boosting achieves causal alignment\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  CONSTRAINT INSUFFICIENT: Consider stronger causal regularization\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
